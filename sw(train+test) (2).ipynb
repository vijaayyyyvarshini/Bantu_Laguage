{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLuUAl_wgbqR",
        "outputId": "90487172-27c0-4498-de7a-80c7aa20a8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbiqHpfBS2QT",
        "outputId": "cc1b39a0-dd6b-4bac-944f-5096bde35df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This defines the vocabulary size. \n",
        "vocabulary_size = 25000"
      ],
      "metadata": {
        "id": "KNeGatkhhS4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This piece of code is taken from Dr. Scannell's notebook\n",
        "import os\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "class BPE_token(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        self.tokenizer.normalizer = Sequence([\n",
        "            NFC()\n",
        "        ])\n",
        "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
        "        self.tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    def bpe_train(self, paths):\n",
        "        trainer = BpeTrainer(vocab_size=vocabulary_size, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
        "            \"<s>\",\n",
        "            \"<pad>\",\n",
        "            \"</s>\",\n",
        "            \"<unk>\",\n",
        "            \"<mask>\"\n",
        "        ])\n",
        "        self.tokenizer.train(paths, trainer)\n",
        "\n",
        "    def save_tokenizer(self, location, prefix=None):\n",
        "        if not os.path.exists(location):\n",
        "            os.makedirs(location)\n",
        "        self.tokenizer.model.save(location, prefix)"
      ],
      "metadata": {
        "id": "WVrsBb8wiBzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = ['sw-train.txt']"
      ],
      "metadata": {
        "id": "VdFimagfiIzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BPE_token()\n",
        "tokenizer.bpe_train(paths)\n",
        "tokenizer.save_tokenizer('.')"
      ],
      "metadata": {
        "id": "aOckXjd8kjwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('.')\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.vocab_size,\n",
        "  bos_token_id=tokenizer.bos_token_id,\n",
        "  eos_token_id=tokenizer.eos_token_id,\n",
        "  n_embd=128,\n",
        "  n_layer=2,\n",
        "  n_head=4\n",
        ")\n",
        "# creating the model\n",
        "model = TFGPT2LMHeadModel(config)\n",
        "model(model.dummy_inputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmw-ioB7k5cZ",
        "outputId": "e3352aab-0eef-4866-9321-bfe4a5be8f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 3727872   \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,727,872\n",
            "Trainable params: 3,727,872\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_string = ''\n",
        "for filename in paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "    x = f.read()\n",
        "  single_string += x + tokenizer.eos_token\n",
        "string_tokenized = tokenizer.encode(single_string)"
      ],
      "metadata": {
        "id": "Nun17n47VAmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_tokenized[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvM2j9zF8akf",
        "outputId": "45ef4836-b622-4b23-ddf9-5f3254d4e813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[86, 466, 3305, 439, 65, 3637, 10, 65, 679, 768]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "goCOT7lC88ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining our optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, epsilon=0.9, clipnorm=1.0)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ],
      "metadata": {
        "id": "6WlmiNjd8_Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 1\n",
        "history = model.fit(dataset, epochs=num_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtEE3hg19NVL",
        "outputId": "d8ec07cf-b3f4-497a-e0dd-66697faad0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6604/6604 [==============================] - 7131s 1s/step - loss: 7.5504 - accuracy: 0.0731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paths_test= ['sw-test.txt']"
      ],
      "metadata": {
        "id": "RqQwzWM_yykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=paths_test\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 50,\n",
        "  num_beams = 5,\n",
        "  temperature = 0.4,\n",
        "  no_repeat_ngram_size=2,\n",
        "  num_return_sequences=5\n",
        ")\n",
        "print(tokenizer.decode(beam_output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAMtLKKfyYc0",
        "outputId": "1935b7fe-d2bf-4766-aff7-f038b7580ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> wa mwaka wa kwanza, na watu wa mkoa wa wakazi wa nchi yake. kwa sababu ya nchi ya kwanza. mwaka mwaka huu. ni mji wa serikali ya serikali, kwa nchi hiyo. lakini ni mwaka mkuu wa tanzania. kuna watu, lakini kwa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_test = BPE_token()\n",
        "tokenizer_test.bpe_train(paths_test)\n",
        "tokenizer_test.save_tokenizer('.')"
      ],
      "metadata": {
        "id": "ncKmbvA-zKof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "# loading tokenizer from the saved model path\n",
        "tokenizer_test = GPT2Tokenizer.from_pretrained('.')\n",
        "tokenizer_test.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer_test.vocab_size,\n",
        "  bos_token_id=tokenizer_test.bos_token_id,\n",
        "  eos_token_id=tokenizer_test.eos_token_id,\n",
        "  n_embd=128,\n",
        "  n_layer=2,\n",
        "  n_head=4\n",
        ")\n",
        "# creating the model\n",
        "model_test = TFGPT2LMHeadModel(config)\n",
        "model_test(model_test.dummy_inputs)\n",
        "model_test.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AezLwKdB0qAG",
        "outputId": "fdc2d5b0-062f-46b8-831a-30570b82f28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 3727872   \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,727,872\n",
            "Trainable params: 3,727,872\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_string_test = ''\n",
        "for filename in paths_test:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "    x = f.read()\n",
        "  single_string_test += x + tokenizer_test.eos_token\n",
        "string_tokenized_test = tokenizer_test.encode(single_string_test)"
      ],
      "metadata": {
        "id": "L_4y-A6U1NxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_tokenized_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7YJAcYJ1WRj",
        "outputId": "dc7e47a6-dc02-4eac-9f6d-036773e16d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[86, 470, 3265, 436, 65, 4062, 10, 65, 684, 787]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized_test) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized_test[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset_test = dataset_test.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "3Oo85GaN3cr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining our optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1, epsilon=0.8, clipnorm=1.0)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model_test.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ],
      "metadata": {
        "id": "Cv6YHXLS3mHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 1\n",
        "history_test = model_test.fit(dataset_test, epochs=num_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1xCgftY4M-O",
        "outputId": "1c08e582-42db-4f78-95fa-8ee490621576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6695/6695 [==============================] - 7244s 1s/step - loss: 6.7476 - accuracy: 0.1095\n"
          ]
        }
      ]
    }
  ]
}