{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "lzOrI_22vdE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad95bb78-c4c9-4bb8-e32e-3858b929da0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/Colab Notebooks\n",
        "%ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlvJdtLDc4xJ",
        "outputId": "9bb23448-7eb8-4f11-a7f6-c7af56c937da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n",
            "\u001b[0m\u001b[01;34m'Before August 2022'\u001b[0m/         cwe-train.txt     nlp.ipynb\n",
            "'Copy of BaselineRNN.ipynb'  \u001b[01;34m'Decision Tree'\u001b[0m/   sw-train.txt\n",
            "'Copy of KayalPost.ipynb'     merges.txt        vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 25000"
      ],
      "metadata": {
        "id": "gRVoG86Y6Y_k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0AetkU9nu8OD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "class BPE_token(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        self.tokenizer.normalizer = Sequence([\n",
        "            NFC()\n",
        "        ])\n",
        "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
        "        self.tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    def bpe_train(self, paths):\n",
        "        trainer = BpeTrainer(vocab_size=vocabulary_size, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
        "            \"<s>\",\n",
        "            \"<pad>\",\n",
        "            \"</s>\",\n",
        "            \"<unk>\",\n",
        "            \"<mask>\"\n",
        "        ])\n",
        "        self.tokenizer.train(paths, trainer)\n",
        "\n",
        "    def save_tokenizer(self, location, prefix=None):\n",
        "        if not os.path.exists(location):\n",
        "            os.makedirs(location)\n",
        "        self.tokenizer.model.save(location, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paths = ['cwe-train.txt']"
      ],
      "metadata": {
        "id": "QEXfuA4Ky2O0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BPE_token()\n",
        "tokenizer.bpe_train(paths)\n",
        "tokenizer.save_tokenizer('.')"
      ],
      "metadata": {
        "id": "9tkx9egWvrUX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "9iW7bB-D0255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96de6a9-a38b-43c3-a961-b263694771e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 transformers-4.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('.')\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.vocab_size,\n",
        "  bos_token_id=tokenizer.bos_token_id,\n",
        "  eos_token_id=tokenizer.eos_token_id,\n",
        "  n_embd=256,\n",
        "  n_layer=4,\n",
        "  n_head=4\n",
        ")\n",
        "# creating the model\n",
        "model = TFGPT2LMHeadModel(config)\n",
        "model(model.dummy_inputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Bz6lVh6l0on1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5c8b3e-5fde-4c4b-8e27-c6556b35af4d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 7654656   \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,654,656\n",
            "Trainable params: 7,654,656\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_string = ''\n",
        "for filename in paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "    x = f.read()\n",
        "  single_string += x + tokenizer.eos_token\n",
        "string_tokenized = tokenizer.encode(single_string)"
      ],
      "metadata": {
        "id": "9TwLNO891JEL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_tokenized[:10]"
      ],
      "metadata": {
        "id": "FtSToBlx1YUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91d505a-61bb-485c-a066-4304a2399b66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[118, 744, 685, 164, 165, 107, 7166, 9279, 7, 8466]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "X0bVGMJg1viW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining our optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ],
      "metadata": {
        "id": "Bgk5kOXb123H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 15\n",
        "history = model.fit(dataset, epochs=num_epoch)"
      ],
      "metadata": {
        "id": "zKapCbRm17P1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75543b9c-e7f0-40bb-bec6-04ce46dbd83b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "86/86 [==============================] - 113s 1s/step - loss: 9.0570 - accuracy: 0.0624\n",
            "Epoch 2/15\n",
            "86/86 [==============================] - 100s 1s/step - loss: 8.3705 - accuracy: 0.0681\n",
            "Epoch 3/15\n",
            "86/86 [==============================] - 98s 1s/step - loss: 7.7751 - accuracy: 0.0717\n",
            "Epoch 4/15\n",
            "86/86 [==============================] - 98s 1s/step - loss: 7.2733 - accuracy: 0.0749\n",
            "Epoch 5/15\n",
            "86/86 [==============================] - 96s 1s/step - loss: 6.8882 - accuracy: 0.0830\n",
            "Epoch 6/15\n",
            "86/86 [==============================] - 98s 1s/step - loss: 6.6162 - accuracy: 0.0962\n",
            "Epoch 7/15\n",
            "86/86 [==============================] - 97s 1s/step - loss: 6.4387 - accuracy: 0.1100\n",
            "Epoch 8/15\n",
            "86/86 [==============================] - 97s 1s/step - loss: 6.3095 - accuracy: 0.1288\n",
            "Epoch 9/15\n",
            "86/86 [==============================] - 98s 1s/step - loss: 6.2029 - accuracy: 0.1464\n",
            "Epoch 10/15\n",
            "86/86 [==============================] - 97s 1s/step - loss: 6.1065 - accuracy: 0.1576\n",
            "Epoch 11/15\n",
            "86/86 [==============================] - 97s 1s/step - loss: 6.0206 - accuracy: 0.1660\n",
            "Epoch 12/15\n",
            "86/86 [==============================] - 96s 1s/step - loss: 5.9405 - accuracy: 0.1737\n",
            "Epoch 13/15\n",
            "86/86 [==============================] - 97s 1s/step - loss: 5.8655 - accuracy: 0.1793\n",
            "Epoch 14/15\n",
            "86/86 [==============================] - 95s 1s/step - loss: 5.7939 - accuracy: 0.1860\n",
            "Epoch 15/15\n",
            "86/86 [==============================] - 94s 1s/step - loss: 5.7275 - accuracy: 0.1907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"An bhfuil\"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 50,\n",
        "  num_beams = 5,\n",
        "  temperature = 0.4,\n",
        "  no_repeat_ngram_size=2,\n",
        "  num_return_sequences=5\n",
        ")\n",
        "print(tokenizer.decode(beam_output[0]))"
      ],
      "metadata": {
        "id": "c9zd5a9QGOPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50cf474-a3da-4112-dc46-cc78f8bfe5fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>n bhfuil na wanhu wa mulungu. maabaho yesu, mbali wanhu wose, na mulungu, \"vino munhu yoyose ya mulungu kwa ichimu cha chilisito. mbali yesu kawalongela, kwaviya yesu chilisito, ivo yesu. kwaviya wanhu weli na mweye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "output_model_file = os.path.join('.', WEIGHTS_NAME)\n",
        "output_config_file = os.path.join('.', CONFIG_NAME)\n",
        "# save model and model configs\n",
        "model.save_pretrained('.')\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "# save tokenizer\n",
        "tokenizer.save_pretrained('.')"
      ],
      "metadata": {
        "id": "i9hUzpj4P2kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c57baf6-8895-4807-b878-418cb331ca91"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tokenizer_config.json',\n",
              " './special_tokens_map.json',\n",
              " './vocab.json',\n",
              " './merges.txt',\n",
              " './added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can then avoid retraining by loading these files when they exist:\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "model = TFGPT2LMHeadModel.from_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "YFCmFHPiQNRQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "4cad730d-b4ae-4a70-f241-d266be69ce4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-33eaa6fa4b56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# can then avoid retraining by loading these files when they exist:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
          ]
        }
      ]
    }
  ]
}